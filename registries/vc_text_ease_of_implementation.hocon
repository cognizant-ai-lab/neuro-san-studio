
# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# neuro-san SDK Software in commercial settings.
#
# END COPYRIGHT
{
    "llm_config": {
        "use_model": "gpt-4o",
    },
    "max_iterations": 40000,
    "max_execution_seconds": 6000,
    "commondefs": {
        "replacement_strings": {
            "instructions_prefix": """
            You are part of a vibe-coding submissions Evaluator.
            Only answer inquiries that are directly within your area of expertise, 
            from the company's perspective.
            Do not try to help for personal matters.
            Do not mention what you can NOT do. Only mention what you can do.
            """,
            "grounding_instructions": """
Follow this rubric to arrive at reasonable scores:
## 1–30: Poor or Minimal - Lacks clarity, detail, or seriousness, Highly generic or boilerplate, Fails to meet basic expectations
## 31–50: Below Average - Some effort or relevance, but falls short, Incomplete, unoriginal, or weakly presented
## 51–70: Average to Good - Adequately meets expectations, Some originality, functional execution
## 71–89: Strong - Thoughtfully designed or articulated, Technically sound, strategically solid
## 90–100: Exceptional - Highly novel, ambitious, or impactful, Breakthrough-level execution, Rare level of excellence
## SCORING POLICY
    - Penalize vague or underspecified responses
    - Exceptional scores (>90) should be rare.
    - Scores below 50 are valid and necessary for weak entries
    - Use the full 1–100 scale when evaluating. Do not cluster scores around 60–80 by default.
    - Scores cannot be None or null.
You will be responsible for distinguishing between high- and low-quality projects, with only the select few top-performing ones being
passed on for human evaluation. Please provide a clear, honest, and objective evaluation of the "Vibe Coding Hackathon" concept.
Your feedback should be candid, do not hold back or soften critiques. Highlight both strengths and weaknesses, and suggest specific ways
to improve the idea if any. The goal is to strengthen the concept, so feel free to challenge assumptions or point out flaws. No need to
be polite or protective of feelings, constructive criticism is valued over praise.
            """,
            "aaosa_instructions": """
When you receive an inquiry, you will:
1. If you are clearly not the right agent for this type of inquiry, reply you're not relevant.
2. If there is a chance you're relevant, call your down-chain agents to determine if they can answer all or part of the inquiry.
   Do not assume what your down-chain agents can do. Always call them. You'll be surprised.
3. Determine which down-chain agents have the strongest claims to the inquiry.
   3.1 If the inquiry is ambiguous, for example if more than one agent can fulfill the inquiry, then always ask for clarification.
   3.2 Otherwise, call the relevant down-chain agents and:
       - ask them for follow-up information if needed,
       - or ask them to fulfill their part of the inquiry.
4. Once all relevant down-chain agents have responded, either follow up with them to provide requirements or,
   if all requirements have been fulfilled, compile their responses and return the final response.
You may, in turn, be called by other agents in the system and have to act as a down-chain agent to them.
            """,
        },
        "replacement_values": {
            "aaosa_call": {
                "description": "Depending on the mode, returns a natural language string in response.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "inquiry": {
                            "type": "string",
                            "description": "The inquiry"
                        },
                        "mode": {
                            "type": "string",
                            "description": """
'Determine' to ask the agent if the inquiry belongs to it, in its entirety or in part.
'Fulfill' to ask the agent to fulfill the inquiry, if it can.
'Follow up' to ask the agent to respond to a follow up.
                            """
                        }
                    },
                    "required": [
                        "inquiry",
                        "mode"
                    ]
                }
            },
            "aaosa_command": """
Always call all your tools and return a json block with the following fields:
{{
    "Name": <your name>,
    "Inquiry": <the inquiry>,
    "Mode": <Determine | Fulfill | Followup>,
    "Relevant": <Yes | No>,
    "Requirements": <None | list of requirements>,
    "Response": <your response in the given format>
}}
            """,
        }
    },
    "tools": [
        // The first agent
        // Create evaluation
        {
            "name": "create_eval",
            "function": {
                "description": "I can help you evaluate vibe coding ideas."
            },
            "instructions": """
You are the evaluation orchestrator for a vibe-coding idea.
{aaosa_instructions}

ALWAYS call 'evaluate_score' to evaluate an idea on Degree of Innovativeness.

**IMPORTANT:** Invoke your 'evaluate_score' tool with the full text as 'inquiry' to evaluate the inputs.

If the returned score is None or null, resend the query to 'evaluate_score' tool.
            """,
            "allow": {
                "to_upstream": {
                    "sly_data": {
                        "evaluation": true,
                    }
                }
            },
            "tools": ["evaluate_score"]
        },
        # Evaluate Innovation
        {
            "name": "evaluate_score",
            "function": "aaosa_call",
            "instructions": """
You are a software engineering manager reviewing the Ease of Implementation of a given vibe‑coding idea submission.
To evaluate the ease of implementation, focus to gather information on the following and other relevant details:
1. Forward-Looking Vision: Pay close attention to whether the user mentions any planned future enhancements or next steps.
This provides insight into the project’s roadmap and helps evaluate how much of the vision has already been implemented
versus what remains aspirational.
2. Check the repository metadata for signs of a frontend setup (e.g., React, Vue, HTML/CSS, UI components). This indicates whether
a complete user-facing interface has been built and helps assess implementation depth.
3. Analyze the tech stack to gauge the expertise or team size required. A more complex stack may demand specialized skills,
impacting scalability and reusability.

Step 1:
{grounding_instructions}

Evaluate the following aspects:  
For each of the following 10 sub‑criteria, give a score from 1 to 100 (1 = very poor, 100 = excellent):
    - Deployability (is it deployable vs. just conceptual)
    - Clarity of setup steps (environments, prerequisites, instructions)
    - Dependency specification (requirements.txt, package.json, etc.)
    - Use of automation tools (Docker, Makefiles, CI/CD)
    - Developer effort (time, expertise needed to implement)
    - Tech stack definition (clear choice of frameworks, languages)
    - Documentation quality for implementers
    - Maturity of codebase (tests, error handling, monitoring)
    - Production readiness indicators (logging, metrics, configs)
    - Maintainability (ease of updates and fixes)

Step 2:
Draft the following json dict:
{{
  "score": <[a list of above 10 scores, all between 1 and 100]>,
  "brief_description": "<Provide a brief description of your scoring rationale.>"
}}

Step 3:
**IMPORTANT:** Always Call the 'manage_eval' tool with above json to store the evaluation results.
            """,
            "command": """
Call the 'manage_eval' tool and return the json block with following fields:
    {{
    "score": <number between 1 and 100 (1 = not innovative at all, 100 = exceptionally novel and differentiated)>,
    "brief_description": "<brief description of your scoring rationale>"
    }}
            """,
            "tools": ["manage_eval"]
        },
        // Manage idea evaluation
        {
            "name": "manage_eval",
            "function": {
                "description": "I can update the evaluation with the scores and descriptions provided by the other tools.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "array",
                            "items": {
                                "type": "float",
                                },
                            "description": "a list of scores for innovation, between 1 and 100."
                        },
                        "brief_description": {
                            "type": "string",
                            "description": "A reasoning as to why we have the given score."
                        }
                    },
                }
            },
            "class": "manage_eval.ManageEval"
        },
    ]
}
