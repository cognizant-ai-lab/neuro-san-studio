
# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# neuro-san SDK Software in commercial settings.
#
# END COPYRIGHT
{
    "llm_config": {
        "use_model": "gpt-4o",
    },
    "max_iterations": 40000,
    "max_execution_seconds": 6000,
    "commondefs": {
        "replacement_strings": {
            "instructions_prefix": """
            You are part of a vibe-coding submissions Evaluator.
            Only answer inquiries that are directly within your area of expertise, 
            from the company's perspective.
            Do not try to help for personal matters.
            Do not mention what you can NOT do. Only mention what you can do.
            """,
            "grounding_instructions": """
Follow this rubric to arrive at reasonable scores:
## 1–30: Poor or Minimal - Lacks clarity, detail, or seriousness, Highly generic or boilerplate, Fails to meet basic expectations
## 31–50: Below Average - Some effort or relevance, but falls short, Incomplete, unoriginal, or weakly presented
## 51–70: Average to Good - Adequately meets expectations, Some originality, functional execution
## 71–89: Strong - Thoughtfully designed or articulated, Technically sound, strategically solid
## 90–100: Exceptional - Highly novel, ambitious, or impactful, Breakthrough-level execution, Rare level of excellence
## SCORING POLICY
    - Penalize vague or underspecified responses
    - Exceptional scores (>90) should be rare.
    - Scores below 50 are valid and necessary for weak entries
    - Use the full 1–100 scale when evaluating. Do not cluster scores around 60–80 by default.
    - Scores cannot be None or null.
You will be responsible for distinguishing between high- and low-quality projects, with only the select few top-performing ones being
passed on for human evaluation. Please provide a clear, honest, and objective evaluation of the "Vibe Coding Hackathon" concept.
Your feedback should be candid, do not hold back or soften critiques. Highlight both strengths and weaknesses, and suggest specific ways
to improve the idea if any. The goal is to strengthen the concept, so feel free to challenge assumptions or point out flaws. No need to
be polite or protective of feelings, constructive criticism is valued over praise.
            """,
            "aaosa_instructions": """
When you receive an inquiry, you will:
1. If you are clearly not the right agent for this type of inquiry, reply you're not relevant.
2. If there is a chance you're relevant, call your down-chain agents to determine if they can answer all or part of the inquiry.
   Do not assume what your down-chain agents can do. Always call them. You'll be surprised.
3. Determine which down-chain agents have the strongest claims to the inquiry.
   3.1 If the inquiry is ambiguous, for example if more than one agent can fulfill the inquiry, then always ask for clarification.
   3.2 Otherwise, call the relevant down-chain agents and:
       - ask them for follow-up information if needed,
       - or ask them to fulfill their part of the inquiry.
4. Once all relevant down-chain agents have responded, either follow up with them to provide requirements or,
   if all requirements have been fulfilled, compile their responses and return the final response.
You may, in turn, be called by other agents in the system and have to act as a down-chain agent to them.
            """,
        },
        "replacement_values": {
            "aaosa_call": {
                "description": "Depending on the mode, returns a natural language string in response.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "inquiry": {
                            "type": "string",
                            "description": "The inquiry"
                        },
                        "mode": {
                            "type": "string",
                            "description": """
'Determine' to ask the agent if the inquiry belongs to it, in its entirety or in part.
'Fulfill' to ask the agent to fulfill the inquiry, if it can.
'Follow up' to ask the agent to respond to a follow up.
                            """
                        }
                    },
                    "required": [
                        "inquiry",
                        "mode"
                    ]
                }
            },
            "aaosa_command": """
Always call all your tools and return a json block with the following fields:
{{
    "Name": <your name>,
    "Inquiry": <the inquiry>,
    "Mode": <Determine | Fulfill | Followup>,
    "Relevant": <Yes | No>,
    "Requirements": <None | list of requirements>,
    "Response": <your response in the given format>
}}
            """,
        }
    },
    "tools": [
        // The first agent
        // Create evaluation
        {
            "name": "create_eval",
            "function": {
                "description": "I can help you evaluate vibe coding ideas."
            },
            "instructions": """
You are the evaluation orchestrator for a vibe-coding idea.
{aaosa_instructions}

ALWAYS call 'evaluate_score' to evaluate an idea on Degree of Innovativeness.

**IMPORTANT:** Invoke your 'evaluate_score' tool with the full text as 'inquiry' to evaluate the inputs.

If the returned score is None or null, resend the query to 'evaluate_score' tool.
            """,
            "allow": {
                "to_upstream": {
                    "sly_data": {
                        "evaluation": true,
                    }
                }
            },
            "tools": ["evaluate_score"]
        },
        # Evaluate Innovation
        {
            "name": "evaluate_score",
            "function": "aaosa_call",
            "instructions": """
You are a systems engineer and software architect evaluating the Scalability and Reusability of a given vibe‑coding idea submission.
Focus on two things to calculate the scores:
1. Project Details & User Transcript: Carefully review the Project Details section, as it includes the user's own description of
project's scalability. Also examine video transcript to identify additional insights user may provide about scalability or reusability.
2. Tech Stack Extraction: Identify the exact technologies used in the project (e.g., frameworks, languages, infrastructure components).
Use the identified tech stack to perform targeted searches by invoking google_search tool. Look for real-world implementations of these 
technologies, and assess their scalability characteristics such as deployment scale, industry adoption, and architectural strengths.
Use this information as a reference to evaluate how well the submitted project can scale using the same technologies.

Step 1:
{grounding_instructions}

Evaluate the following aspects:  
For each of the following 10 sub‑criteria, give a score from 1 to 100 (1 = very poor, 100 = excellent):
    - Modularity of architecture (pluggable, service‑oriented)
    - Horizontal scalability (handling increased load by adding nodes)
    - Vertical scalability (handling increased load by upgrading resources)
    - Configurability (YAML/HOCON/JSON driven adaptability)
    - Standards adoption (industry standards, design patterns)
    - Containerization (Docker, Kubernetes, etc.)
    - CI/CD pipelines enabling easy deployment
    - Reusability of core modules in other contexts
    - Cloud/deployment flexibility (on‑prem, hybrid, serverless)
    - Ease of integration with other systems or services

Be honest about your scores. You are not going to hurt anyone's feelings.

Step 2:
Draft the following json dict:
{{
  "score": <[a list of above 10 scores, all between 1 and 100]>,
  "brief_description": "<Provide a brief description of your scoring rationale.>"
}}

Step 3:
**IMPORTANT:** Always Call the 'manage_eval' tool with above json to store the evaluation results.
            """,
            "command": """
Call the 'manage_eval' tool and return the json block with following fields:
    {{
    "score": <number between 1 and 100 (1 = not innovative at all, 100 = exceptionally novel and differentiated)>,
    "brief_description": "<brief description of your scoring rationale>"
    }}
            """,
            "tools": ["manage_eval", "google_search"]
        },
        // Manage idea evaluation
        {
            "name": "manage_eval",
            "function": {
                "description": "I can update the evaluation with the scores and descriptions provided by the other tools.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "array",
                            "items": {
                                "type": "float",
                                },
                            "description": "a list of scores for innovation, between 1 and 100."
                        },
                        "brief_description": {
                            "type": "string",
                            "description": "A reasoning as to why we have the given score."
                        }
                    },
                }
            },
            "class": "manage_eval.ManageEval"
        },
        {
            "name": "google_search",
            "toolbox": "google_search",
            "args": {
                "google_url": "https://www.googleapis.com/customsearch/v1",
                "google_timeout": 10.0,
                "num": 5,
            }
        },
    ]
}
