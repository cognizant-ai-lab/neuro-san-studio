
# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# neuro-san SDK Software in commercial settings.
#
# END COPYRIGHT
{
    "llm_config": {
        "use_model": "gpt-4o",
    },
    "max_iterations": 40000,
    "max_execution_seconds": 6000,
    "commondefs": {
        "replacement_strings": {
            "instructions_prefix": """
            You are part of a vibe-coding submissions Evaluator.
            Only answer inquiries that are directly within your area of expertise, 
            from the company's perspective.
            Do not try to help for personal matters.
            Do not mention what you can NOT do. Only mention what you can do.
            """,
            "grounding_instructions": """
Follow this rubric to arrive at reasonable scores:
## 1–30: Poor or Minimal - Lacks clarity, detail, or seriousness, Highly generic or boilerplate, Fails to meet basic expectations
## 31–50: Below Average - Some effort or relevance, but falls short, Incomplete, unoriginal, or weakly presented
## 51–70: Average to Good - Adequately meets expectations, Some originality, functional execution
## 71–89: Strong - Thoughtfully designed or articulated, Technically sound, strategically solid
## 90–100: Exceptional - Highly novel, ambitious, or impactful, Breakthrough-level execution, Rare level of excellence
## SCORING POLICY
    - Penalize vague or underspecified responses
    - Exceptional scores (>90) should be rare.
    - Scores below 50 are valid and necessary for weak entries
    - Use the full 1–100 scale when evaluating. Do not cluster scores around 60–80 by default.
    - Scores cannot be None or null.
You will be responsible for distinguishing between high- and low-quality projects, with only the select few top-performing ones being
passed on for human evaluation. Please provide a clear, honest, and objective evaluation of the "Vibe Coding Submission Idea".
Your feedback should be candid, do not hold back or soften critiques. Highlight both strengths and weaknesses, and suggest specific ways
to improve the idea if any. The goal is to strengthen the concept, so feel free to challenge assumptions or point out flaws. No need to
be polite or protective of feelings, constructive criticism is valued over praise.
            """,
            "aaosa_instructions": """
When you receive an inquiry, you will:
1. If you are clearly not the right agent for this type of inquiry, reply you're not relevant.
2. If there is a chance you're relevant, call your down-chain agents to determine if they can answer all or part of the inquiry.
   Do not assume what your down-chain agents can do. Always call them. You'll be surprised.
3. Determine which down-chain agents have the strongest claims to the inquiry.
   3.1 If the inquiry is ambiguous, for example if more than one agent can fulfill the inquiry, then always ask for clarification.
   3.2 Otherwise, call the relevant down-chain agents and:
       - ask them for follow-up information if needed,
       - or ask them to fulfill their part of the inquiry.
4. Once all relevant down-chain agents have responded, either follow up with them to provide requirements or,
   if all requirements have been fulfilled, compile their responses and return the final response.
You may, in turn, be called by other agents in the system and have to act as a down-chain agent to them.
            """,
        },
        "replacement_values": {
            "aaosa_call": {
                "description": "Depending on the mode, returns a natural language string in response.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "inquiry": {
                            "type": "string",
                            "description": "The inquiry"
                        },
                        "mode": {
                            "type": "string",
                            "description": """
'Determine' to ask the agent if the inquiry belongs to it, in its entirety or in part.
'Fulfill' to ask the agent to fulfill the inquiry, if it can.
'Follow up' to ask the agent to respond to a follow up.
                            """
                        }
                    },
                    "required": [
                        "inquiry",
                        "mode"
                    ]
                }
            },
            "aaosa_command": """
Always call all your tools and return a json block with the following fields:
{{
    "Name": <your name>,
    "Inquiry": <the inquiry>,
    "Mode": <Determine | Fulfill | Followup>,
    "Relevant": <Yes | No>,
    "Requirements": <None | list of requirements>,
    "Response": <your response in the given format>
}}
            """,
        }
    },
    "tools": [
        // The first agent
        // Create evaluation
        {
            "name": "create_eval",
            "function": {
                "description": "I can help you evaluate vibe coding ideas."
            },
            "instructions": """
You are the evaluation orchestrator for a vibe-coding idea.
{aaosa_instructions}

ALWAYS call 'evaluate_score' to evaluate an idea on Degree of Innovativeness.

**IMPORTANT:** Invoke your 'evaluate_score' tool with the full text as 'inquiry' to evaluate the inputs.

If the returned score is None or null, resend the query to 'evaluate_score' tool.
            """,
            "allow": {
                "to_upstream": {
                    "sly_data": {
                        "evaluation": true,
                    }
                }
            },
            "tools": ["evaluate_score"]
        },
        # Evaluate Innovation
        {
            "name": "evaluate_score",
            "function": "aaosa_call",
            "instructions": """
You are an expert in product innovation and emerging technologies. 
Your job is to assess the Degree of Innovativeness of the vibe coding idea submission based on the input.

Step 1:
Before assigning any scores, you must first gather relevant background context by invoking the following tools:
    1. RAGcontext_innovation — Search through ~200,000 research papers in the computer science domain to
    identify whether similar ideas or techniques have already been proposed or published.
    2. Webcontext_innovation — Search through websites like Google, Twitter, GitHub, Medium, and other startup
    or product directories to find evidence of similar business ideas already implemented or discussed publicly.
    These two sources give context which must be considered to evaluate the innocation score. Summarize your findings clearly.

Step 2:
{grounding_instructions}

Evaluate the following aspects:
For each of the following 10 sub‑criteria, give a score from 1 to 100 (1 = very poor, 100 = excellent):
    - Novelty of concept
    - Uniqueness of implementation
    - Originality compared to existing solutions
    - Clever use of technology or resources
    - Use of proprietary methods or algorithms
    - Research‑backed ideas or patents
    - Integration of cutting‑edge tools (e.g., AI/ML, LLMs)
    - Differentiation from open‑source or market solutions
    - Breadth of components (backend, frontend, infra)
    - Potential to inspire new approaches in the field

Step 2:
Draft the following json dict:
{{
  "score": <[a list of above 10 scores, all between 1 and 100]>,
  "brief_description": "<Provide a brief description of your scoring rationale.>"
}}

Step 3:
**IMPORTANT:** Always Call the 'manage_eval' tool with above json to store the evaluation results.
            """,
            "command": """
Call the 'manage_eval' tool and return the json block with following fields:
    {{
    "score": <number between 1 and 100 (1 = not innovative at all, 100 = exceptionally novel and differentiated)>,
    "brief_description": "<brief description of your scoring rationale>"
    }}
            """,
            "tools": ["manage_eval", "RAGcontext_innovation", "Webcontext_innovation"]
        },
        {
            "name": "RAGcontext_innovation",
            "function": "aaosa_call",
            "instructions": """
You are an expert in extracting context for the LLMs.
Your primary role is to search through approximately 200,000 research papers in the computer science domain to identify whether ideas or techniques
similar to the vibe coding submissions have already been proposed or published.
Step 1:
If document loading is required, invoke the innovation_tool to execute the load_csv action; otherwise, skip this step.
Use the following json format to call the tool:
{{
    "action": "load_csv"
    "folder_path": "<path to the csv folder>"
}}
Step 2:
Invoke the innovation_tool with query_documents action to retrieve ideas from the RAG to use as context for the LLMs. The query parameter must contain
a detailed summary of the idea, highlighting its objective and potential impact, the problems solved, market gap, tools, models, agents, or frameworks
used, relevant AI, machine learning, and data science concepts applied and other details.
Use the following json format to call the tool:
{{
    "action": "query_documents"
    "query": "<brief summary of the user idea>"
}}
Step 3:
After retrieving context from the RAG setup, format it clearly and compactly for the LLM. Please:
- Include all relevant points.
- Be concise, not verbose.
- Use structured formatting (e.g., bullets or sections).
- Ensure clarity and completeness to aid LLM understanding.
            """,
            "tools": ["innovation_tool"]
        },
        {
            "name": "Webcontext_innovation",
            "function": "aaosa_call",
            "instructions": """
You are an expert in product innovation and emerging technologies. 
Your job is to find the Web context and assess the Webcontext_innovation_score of the vibe coding idea submission based on the input.

Step 1:
Using the google_search tool to retrieve similar ideas from the web which you can use as context to evaluate how innovative is the idea given to you
and calculate a Webcontext_innovation_score.
Please keep in mind that we need to complete this search as efficiently as possible.
USE A MAXIMUM OF ONE GOOGLE SEARCH QUERY.
We need to call the query to consider the following three cases:
1. Projects/Businesses that are very similar in both idea and implementation to the submitted idea.
2. Implementations that use the same core technology stack or methods but solve problems in a different domain or serve a different market.
3. Projects targeting the same user need or domain, but achieving it with a different methodology or technical architecture.

Step 2:
After retrieving context from the google_search setup, format it clearly and compactly for the LLM. Please:
- Include all relevant points.
- Be concise, not verbose.
- Use structured formatting (e.g., bullets or sections).
- Ensure clarity and completeness to aid LLM understanding.
            """,
            "tools": ["google_search"]
        },
        // Manage idea evaluation
        {
            "name": "manage_eval",
            "function": {
                "description": "I can update the evaluation with the scores and descriptions provided by the other tools.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "array",
                            "items": {
                                "type": "float",
                                },
                            "description": "a list of scores for innovation, between 1 and 100."
                        },
                        "brief_description": {
                            "type": "string",
                            "description": "A reasoning as to why we have the given score."
                        }
                    },
                }
            },
            "class": "manage_eval.ManageEval"
        },
        {
            "name": "innovation_tool",
            "function": {
                "description": "Performs semantic search over arXiv to find research ideas similar to a user's input, supporting innovation score evaluation.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "action": {
                            "type": "string",
                            description: "The action to perform: 'load_csv' or 'query_documents'."
                        },
                        "folder_path": {
                            "type": "string",
                            description: "Path to folder with CSVs. Needed for 'load_csv'."
                        },
                        "query": {
                            "type": "string",
                            "description": "Query string for document search. Needed for 'query_documents'."
                        },
                    },
                    required: ["action"]
                }
            },
            "class": "innovation_tool.ArxivRAG"
        },
        {
            "name": "google_search",
            "toolbox": "google_search",
            "args": {
                "google_url": "https://www.googleapis.com/customsearch/v1",
                "google_timeout": 10.0,
                "num": 5,
            }
        },
    ]
}
